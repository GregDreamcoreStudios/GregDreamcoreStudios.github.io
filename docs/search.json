[
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "GregsBlog",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nPost With Code\n\n\n\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\nJan 26, 2024\n\n\nHarlow Malloc\n\n\n\n\n\n\n\n\n\n\n\n\nWelcome To My Blog\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\nJan 23, 2024\n\n\nTristan O’Malley\n\n\n\n\n\n\n\n\n\n\n\n\nThe Tech Talent Shuffle: Redrawing Silicon Valley’s Job Map\n\n\nDo hiring anomalies in Big Tech suggest they’re taking non-Analytical employees for granted?\n\n\n\n\n\n\n\n\nJan 24, 2023\n\n\nGregory Sharma\n\n\n\n\n\n\n\n\n\n\n\n\nThis is a dummy blog posts\n\n\n\n\n\n\n123\n\n\nSecond Tag\n\n\n\nThis is a test post. In this post, I try out different functionalities\n\n\n\n\n\nJun 1, 2022\n\n\nAlbert Rapp\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/newsletter/Newsletter.html",
    "href": "posts/newsletter/Newsletter.html",
    "title": "The Tech Talent Shuffle: Redrawing Silicon Valley’s Job Map",
    "section": "",
    "text": "Variable hiring trends in big tech with fastest growth in technical roles (Engineers, Scientists).\n\n\nSlower growth in Sales, Marketing, and Administrative roles; recent net outflows noted.\n\n\nEducational background influences disparities in hiring trends and salaries across job categories, more so for non-technical roles.\n\n\nDecline in employee satisfaction ratings since 2020, particularly for Operations roles.\n\n\nOperations and Marketing job categories may have predictive value for company equity returns, with operations inflows and marketing outflows marking lower and higher returns, respectively.\nCode\n# import functions\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport plotly\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\nimport plotly.express as px\nfrom plotly.offline import plot\nimport warnings\n\nimport statsmodels.api as sm\nCode\nproj_directory_path = r\"C:\\Users\\Greg\\Desktop\\testing_newsletter\" # the path to your project directory\n\n# individual_position, individual_education, role_breakdown, and individual_raw_reviews should be in this folder as unzipped folders\n\n# the following merges all csvs for these folders and writes them to the proj_directory folder\nCode\nfrom os import listdir\nfrom os.path import isfile, join\n\n\n#------------------------------individual data------------------------------#\n\n# individual_data_types = [\"position\",\n#                         \"education\"]\n\n# for individual_data_type in individual_data_types:\n#         mypath = f\"{proj_directory_path}/individual_{individual_data_type}\"    # path to folder with all the csv files, e.g. /invidual_position\n#         csv_files = [f\"{mypath}/{f}\" for f in listdir(mypath) if isfile(join(mypath, f))]\n#         df = pd.concat([pd.read_csv(f) for f in csv_files ], ignore_index=True)\n#         df.to_csv(f\"{proj_directory_path}/individual_{individual_data_type}_merged.csv\")   # output merged file, e.g., individual_position_merged.csv\n\n# # #------------------------------workfore dynamics role breakdown data------------------------------#\n\n# breakdown_types = [\"role\"]\n\n# for breakdown_type in breakdown_types:\n#         mypath = f\"{proj_directory_path}/{breakdown_type}_breakdown\"    # path to folder with all the csv files, e.g. /role_breakdown\n#         csv_files = [f\"{mypath}/{f}\" for f in listdir(mypath) if isfile(join(mypath, f))]\n#         df = pd.concat([pd.read_csv(f) for f in csv_files ], ignore_index=True)\n#         df.to_csv(f\"{proj_directory_path}/{breakdown_type}_breakdown_merged.csv\")   # output merged file, e.g., role_breakdown_merged.csv\n\n# #------------------------------sentiment data ------------------------------#\n\n# mypath = f\"{proj_directory_path}/individual_raw_reviews\"    # path to folder with all the csv files, /sentiment_data\n# csv_files = [f\"{mypath}/{f}\" for f in listdir(mypath) if isfile(join(mypath, f))]\n# df = pd.concat([pd.read_csv(f) for f in csv_files ], ignore_index=True)\n# df.to_csv(f\"{proj_directory_path}/sentiment_data_merged.csv\")   # output merged file, sentiment_data_merged.csv\nCode\ninclude_plotlyjs=True\nCode\n# plot dimensions for all plots\nmy_width = 1000\nmy_height = int(my_width*0.6)"
  },
  {
    "objectID": "posts/newsletter/Newsletter.html#netflows",
    "href": "posts/newsletter/Newsletter.html#netflows",
    "title": "The Tech Talent Shuffle: Redrawing Silicon Valley’s Job Map",
    "section": "1. Netflows",
    "text": "1. Netflows\nBig tech companies have consistently grown their human capital in the last decade. However, these hiring inflows have recently been more discriminatory with respect to job category. To better understand these effects, we use our role breakdown dataset and considering the following basket of companies:\n\nMicrosoft, Alphabet, Apple, Meta, Amazon, Netflix\n\nWe compute a monthly net inflow value for each company, “net flow”, taken as the change in our estimated “count” of the number of employees employed within that job category in North America.\nSince the ‘budget’ for human capital in each job category varies among these companies, naturally due to their business nature, we also compute percentage change values to minimize the impact of these budget differences. This gives us a better picture of growth in hiring in these categories as it pertains to individuals, not the company as a whole. See Figure 1.\nWe find that, when aggregating these net flows across all companies in our basket:\n\nEngineer and Scientist roles are growing at the fastest rates, &gt; 1% per month!\nSales (0.56%) and Marketing (0.82%) roles are growing the slowest at nearly half their technical counterparts!\n\nIn terms of raw inflows, or the raw net number of people hired in these categories:\n\nJust under 3000 net Engineers are acquired per month, at nearly triple the next largest category, Sales (890).\nFor financiers and scientists, inflows have been minimal despite having average and highest growth rates, respectively.\n\n\n\nCode\nrole_df = pd.read_csv(f\"{proj_directory_path}/role_breakdown_merged.csv\") # read file\n\nrole_df['month'] = pd.to_datetime(role_df['month'], format=\"%Y-%m\") # get month to datetime\n\nnp.seterr(divide = 'ignore')  #ignore annoying warnings\n\nrole_df['netflow'] = role_df['inflow'] - role_df['outflow'] # create netflow column\nrole_df['log_count'] = np.where(role_df['count'] &gt; 0, np.log(role_df['count']), np.nan) # take log of counts\n\nrole_df['log_inflow'] = np.where(role_df['outflow'] &gt; 0, np.log(role_df['outflow']), np.nan)      # these are not really useful, it was an initial idea\nrole_df['log_outflow'] = np.where(role_df['outflow'] &gt; 0, np.log(role_df['outflow']), np.nan)      # these are not really useful, it was an initial idea\nrole_df['log_netflow'] = role_df['log_inflow'] - role_df['log_outflow']      # these are not really useful, it was an initial idea\n                                  \n#role_df['log_salary'] = np.log(role_df['salary']) # get log of salary\n#role_df.head()\n\n\n\n\nCode\n# filter role_df such that region is only N.A., and job_category is not NaN\nsample = role_df[(role_df['region'] == \"Northern America\") & (role_df['job_category'] != \"empty\")] # only NA with job_category data\n\n#drop non-numeric columns so we can get groupby means\nsample = sample.drop(columns=[\"Unnamed: 0\", \"rcid\", \"region\", \"role_k50\", \"role_k150\"])   # don't need these cols\n\n\n\n\nCode\n# consistent color map for plots\njob_category_coloring = {'Admin': '#636efa', 'Engineer': '#EF553B', 'Finance': '#00cc96', 'Marketing': '#ab63fa', 'Operations': '#FFA15A', 'Sales': '#19d3f3', 'Scientist': '#FF6692'}\n\n\n\n\nCode\nmean_netflow_monthly = sample.groupby(['month', 'job_category'])[['count']].sum()          # group our data by month and job_category\n\nmean_netflow_monthly['shifted_count'] = mean_netflow_monthly.groupby('job_category')['count'].shift(1)   # shift count \nmean_netflow_monthly['netflow'] = mean_netflow_monthly['count'] - mean_netflow_monthly['shifted_count']    # get differences in count, I decided to prefer this over inflow-outflow\nmean_netflow_monthly['log_netflow'] = np.log(mean_netflow_monthly['count']) - np.log(mean_netflow_monthly['shifted_count']) # log change in count\nmean_netflow_monthly['log_count'] = np.log(mean_netflow_monthly['count'])  # take log of count\nmean_netflow_monthly = mean_netflow_monthly.drop(columns=['shifted_count'])     # don't need this anymore\nmean_netflow_monthly = mean_netflow_monthly.dropna(subset=['netflow', 'log_netflow'])     # dropna for these guys\n\n\nmean_netflow_monthly.reset_index(inplace=True)                                                    # reset_index to easily access columns\n#mean_netflow_monthly\n\n\n\n\nCode\n# Assuming 'mean_netflow_monthly' and 'job_category_coloring' are already defined\n\n# Assuming mean_netflow_monthly is your DataFrame\ncategories = mean_netflow_monthly['job_category'].unique()\nmean_diff_logs = {}\n\n# Calculate mean_diff_log for each category\nfor category in categories:\n    filtered_df = mean_netflow_monthly[mean_netflow_monthly['job_category'] == category]\n    mean_diff_log = filtered_df['count'].pct_change().mean()       # get the average % change in monthly counts for all job categories, disregard the _log, these are % chngs\n    mean_diff_logs[category] = mean_diff_log\n\n# Sort categories by mean_diff_log\nsorted_categories = sorted(mean_diff_logs, key=mean_diff_logs.get)\n\n# Create traces for the mean difference of Log Netflow\nlog_diff_traces = []\nfor category in sorted_categories:\n    log_diff_traces.append(go.Bar(\n        x=[category],\n        y=[mean_diff_logs[category]],\n        name=category,\n        marker_color=job_category_coloring[category]\n    ))\n\n\nmean_diff_netflows = {}\nfor category in categories:\n    filtered_df = mean_netflow_monthly[mean_netflow_monthly['job_category'] == category]\n    mean_diff_netflow = filtered_df['count'].diff().mean()    # get the average monthly change in counts\n    mean_diff_netflows[category] = mean_diff_netflow\n\n# Sort categories by mean_diff_netflow\nsorted_categories_netflow = sorted(mean_diff_netflows, key=mean_diff_netflows.get)\n\n# Create traces for the mean difference of Netflow using sorted categories\nnetflow_diff_traces = []\nfor category in sorted_categories_netflow:\n    netflow_diff_traces.append(go.Bar(\n        x=[category],\n        y=[mean_diff_netflows[category]],\n        name=category,\n        marker_color=job_category_coloring[category],\n        visible=False  # Initially hidden\n    ))\n\n# Combine all traces\nfig = go.Figure(data=log_diff_traces + netflow_diff_traces)\n\n# Update layout\nfig.update_layout(\n    title=\"Mean Monthly Netflows w.r.t. Job Category\",\n    xaxis_title='Job Category',\n    yaxis_title=\"Mean Monthly Chng. in Netflow\",\n    legend_title='Job Category',\n    xaxis_tickangle=-45,\n    width=my_width,\n    height=my_height,\n    updatemenus=[\n        dict(\n            type='buttons',\n            direction='right',\n            x=0.7,\n            y=1.6,\n            showactive=True,\n            buttons=[\n                dict(label='% Chg.',\n                     method='update',\n                     args=[{'visible': [True if i &lt; len(log_diff_traces) else False for i in range(len(log_diff_traces + netflow_diff_traces))]},\n                           {'yaxis': {'title': 'Mean Monthly % Chng. in Employees', 'tickformat': ',.2%'}}]),\n                dict(label='Raw Netflow',\n                     method='update',\n                     args=[{'visible': [False if i &lt; len(log_diff_traces) else True for i in range(len(log_diff_traces + netflow_diff_traces))]},\n                           {'yaxis': {'title': 'Mean Monthly Chng. in Employees', 'tickformat': ''}}])\n            ]\n        )\n    ]\n)\n\n# Show the figure\nfig.show(renderer=\"notebook\")\n\n\n\n\n                                                \n\n\nFigure 1: Mean Log/Raw Net Inflows w.r.t. Job Category"
  },
  {
    "objectID": "posts/newsletter/Newsletter.html#cumulative-net-inflows-vs.-job-category",
    "href": "posts/newsletter/Newsletter.html#cumulative-net-inflows-vs.-job-category",
    "title": "The Tech Talent Shuffle: Redrawing Silicon Valley’s Job Map",
    "section": "2. Cumulative Net Inflows Vs. Job Category",
    "text": "2. Cumulative Net Inflows Vs. Job Category\nConsidering only the average net hiring in these categories offers little insight into time-varying trends. To grasp hiring in these categories over time, we clustered our net flow dataset by category and month. Hiring growth in these categories has been practically identical in nature (covarying) since 2016 - until 2023, that is. See Figure 2.\nIn 2023, net hiring has been positive only in Finance and Scientist roles. 2023 is the first year since the beginning of our dataset, 2016, where marketing, sales, and administrator roles have experienced net outflows.\nAll these effects seem to have started in mid-2022 (except for Operations roles, who weren’t cool enough for the party as early as mid-2021!).\n\n\nCode\n# Assuming 'mean_netflow_monthly' and 'job_category_coloring' are already defined\n\n# Create traces for Cumulative Log Netflow\nlog_traces = []\nfor category in mean_netflow_monthly['job_category'].unique():\n    filtered_df = mean_netflow_monthly[mean_netflow_monthly['job_category'] == category]\n    log_traces.append(go.Scatter(\n        x=filtered_df['month'],\n        y=filtered_df['log_count'],     # plot log count over time for each job category\n        name=category,\n        marker_color=job_category_coloring[category],\n        mode='markers+lines'\n    ))\n\n# Create traces for Cumulative Netflow\nnetflow_traces = []\nfor category in mean_netflow_monthly['job_category'].unique():\n    filtered_df = mean_netflow_monthly[mean_netflow_monthly['job_category'] == category]\n    netflow_traces.append(go.Scatter(\n        x=filtered_df['month'],\n        y=filtered_df['count'],            # plot raw count over time for each job category\n        name=category,\n        marker_color=job_category_coloring[category],\n        mode='markers+lines',\n        visible=False  # Initially hidden\n    ))\n\n# Create traces for Normalized Log Netflow\nlog_norm_traces = []\nfor category in mean_netflow_monthly['job_category'].unique():\n    filtered_df = mean_netflow_monthly[mean_netflow_monthly['job_category'] == category]\n    log_norm_traces.append(go.Scatter(\n        x=filtered_df['month'],\n        y=filtered_df['log_count'] - filtered_df.loc[filtered_df.index[0], 'log_count'],   # shift down the log_counts by their first value to start at 0 in 2016 for easy comparison\n        name=category,   \n        marker_color=job_category_coloring[category],\n        mode='markers+lines',\n        visible=False  # Initially hidden\n    ))\n\n# Combine all traces\nfig = go.Figure(data=log_traces + netflow_traces + log_norm_traces)\n\n# Update layout\nfig.update_layout(\n    title='Cumulative Netflow w.r.t. Job Category in Northern America',\n    xaxis_title='Month',\n    yaxis_title='Cumulative Netflow',\n    legend_title='Job Category',\n    xaxis_tickangle=-45,\n    width=my_width,\n    height=my_height,\n    updatemenus=[\n        dict(\n            type='buttons',\n            direction='right',\n            x=0.7,\n            y=1.6,\n            showactive=True,\n            buttons=[\n                dict(label='Log',\n                     method='update',\n                     args=[{'visible': [True if i &lt; len(log_traces) else False for i in range(len(log_traces + netflow_traces + log_norm_traces))]},\n                           {'yaxis': {'title': 'Cumulative Log Netflow'}}]),\n                dict(label='Netflow',\n                     method='update',\n                     args=[{'visible': [False if i &lt; len(log_traces) or i &gt;= len(log_traces) + len(netflow_traces) else True for i in range(len(log_traces + netflow_traces + log_norm_traces))]},\n                           {'yaxis': {'title': 'Cumulative Netflow'}}]),\n                dict(label='Log_Norm',\n                     method='update',\n                     args=[{'visible': [False if i &lt; len(log_traces) + len(netflow_traces) else True for i in range(len(log_traces + netflow_traces + log_norm_traces))]},\n                           {'yaxis': {'title': 'Normalized Cumulative Log Netflow'}}])\n            ]\n        )\n    ]\n)\n\n# Show the figure\nfig.show()\n\n\n\n\n                                                \n\n\nFigure 2: Cumulative Net Inflows w.r.t. Job Category"
  },
  {
    "objectID": "posts/newsletter/Newsletter.html#a.-us-university-rank-discrimination",
    "href": "posts/newsletter/Newsletter.html#a.-us-university-rank-discrimination",
    "title": "The Tech Talent Shuffle: Redrawing Silicon Valley’s Job Map",
    "section": "3a. US University Rank Discrimination",
    "text": "3a. US University Rank Discrimination\nHowever, do these inflow effects truly indicate a difficulty in getting hired? To explore the differences between job categories amongst these tech companies on the basis of education, we took a look at all individuals in Revelio’s individual position and education dataset employed by these companies. Clustering average salary (as per Revelio’s salary prediction model for job positions) by the rank of one’s university in the United States, and grouping by job category, we find the following:\nSalary differences are relatively uniform among those who attended below-average universities:\n\nEngineers &gt; Financiers &gt; Marketers &gt; Administrators &gt; Scientists &gt; Operations\n\nAmong those who attended above-average universities, linearly higher salaries are observed for those with higher university rankings.\n\nThese linearly increasing salaries grow similarly amongst all job categories, with one notable exception: We observe that Scientist salaries grow uniquely faster than all other categories in this upper half of the dataset.\n\nLastly, for those who attended prestigious universities:\n\nDifferences in salaries among job categories shrink dramatically, yet even within this upper academic echelon, the hierarchy of salaries is still observed.\n\n\n\nCode\nbig_boys = role_df.company.unique().tolist() # our big tech companies\n# print(big_boys)\n\njob_categories = role_df.job_category.unique().tolist()[:-1] # ignore #empty job_category\n#job_categories.pop()\n# print(job_categories)\n\n\n\n\nCode\nwarnings.filterwarnings('ignore')\n\nposition_df = pd.read_csv(f\"{proj_directory_path}/individual_position_merged.csv\")\neducation_df = pd.read_csv(f\"{proj_directory_path}/individual_education_merged.csv\")\n\n\n\n\nCode\nposition_df = position_df[(position_df['country'] == \"United States\") & position_df['job_category'].isin(job_categories)]   # only US and non NaN job categories\nposition_df = position_df[position_df['ultimate_parent_company_name'].isin(big_boys)]   # only big boys\n\n\n\n\nCode\nposition_df['log_salary'] = np.log(position_df['salary']) # get log salary of salary\nposition_df['startdate'] = pd.to_datetime(position_df['startdate'], format=\"%Y-%m-%d\")    # datetime conversions\nposition_df['enddate'] = pd.to_datetime(position_df['enddate'], format=\"%Y-%m-%d\")        # datetime conversions\nposition_df['start_month'] = position_df['startdate'].dt.strftime('%Y-%m')   \nposition_df['start_year'] = position_df['startdate'].dt.strftime('%Y')\n\n\n\n\nCode\nposition_df.set_index('user_id', inplace=True)    # set index for position_df to join us_rank values\n\nrank_df = education_df[['user_id', 'us_rank']]   # filter down education df to add it to position_df \nrank_df = rank_df.drop_duplicates(subset='user_id', keep='first')   # drop duplicate us_ranks for each user_id\nrank_df.set_index('user_id', inplace=True)    # set index for joining\n\nposition_df = position_df.join(rank_df, how='left')\n\nposition_df = position_df[position_df[['salary', 'us_rank']].notnull().all(1)]   # drop rows where salary and us_rank are NaN\noutput = position_df.groupby(['us_rank', 'job_category'], as_index=False)['log_salary'].mean().sort_values(by='us_rank', ascending=False)  #gropuby means\noutput = output[output['log_salary'] &gt; 10] # one outlier at 6 when rest of data between 10 and 12\n\n\n\n\nCode\n# Assuming 'output' and 'job_category_coloring' are already defined\nfig = px.scatter(output,\n                 x='us_rank', \n                 y='log_salary', \n                 color='job_category',\n                 trendline=\"lowess\",\n                 color_discrete_map=job_category_coloring,\n                 title='Salary w.r.t. US Rank for All Job Categories in Big Boys',\n                 labels={'mult': 'Average Salary', 'us_rank': 'University Rank in US'})\n\n# Convert to a graph objects figure\nfig = go.Figure(fig)\n\n# Update layout for better readability\nfig.update_layout(\n    xaxis_title='US_Rank',\n    yaxis_title='Average Salary',\n    legend_title='Job Category',\n    xaxis_tickangle=-45,\n    width=my_width,\n    height=my_height\n)\n\n# Update marker and trendline attributes\nfor i, trace in enumerate(fig.data):\n    if trace.mode == 'markers':\n        trace.marker.size = 4\n        trace.hoverinfo = 'skip'\n        trace.hovertemplate = None \n    elif trace.mode == 'lines':\n        trace.line.width = 8\n        trace.showlegend = True\n\n# Create toggle buttons for trendlines\nbuttons = []\nall_lines_visible = True  # Initial state where all lines are visible\n\n# Add \"All\" button\nbuttons.append(dict(method='restyle',\n                    label='All',\n                    args=[{'visible': [True if t.mode != 'lines' or all_lines_visible else False for t in fig.data]}],\n                    args2=[{'visible': [True if t.mode != 'lines' else not all_lines_visible for t in fig.data]}]\n                    ))\n\nfor i, trace in enumerate(fig.data):\n    if trace.mode == 'lines':\n        buttons.append(dict(method='restyle',\n                            label=trace.name,\n                            args=[{'visible': [t.mode != 'lines' or t.name == trace.name for t in fig.data]}],\n                            args2=[{'visible': [t.mode != 'lines' or (t.name != trace.name and t.mode == 'lines') for t in fig.data]}]\n                            ))\n\n# Add buttons to layout\nfig.update_layout(\n    updatemenus=[\n        dict(\n            type='buttons',\n            direction='right',\n            x=0.7,\n            y=1.6,\n            showactive=True,\n            buttons=buttons\n        )\n    ]\n)\n\n# Show the figure\nfig.show()"
  },
  {
    "objectID": "posts/newsletter/Newsletter.html#b.-employee-ratings",
    "href": "posts/newsletter/Newsletter.html#b.-employee-ratings",
    "title": "The Tech Talent Shuffle: Redrawing Silicon Valley’s Job Map",
    "section": "3b. Employee Ratings",
    "text": "3b. Employee Ratings\nRatings for all roles in these companies have also declined since 2020! Interestingly, the hierarchy of these job categories in terms of relative average rating largely agrees with that of the salary differences we just saw.\nOperations employees have consistently had the worst experience, not to mention their lower relative salary as identified above. It is highly likely that it is not a coincidence these employees have been leaving big tech the most!\n\n\nCode\nsentiment_df = position_df = pd.read_csv(f\"{proj_directory_path}/sentiment_data_merged.csv\")\n\nbig_boys = ['Amazon.com, Inc.', 'Microsoft Corp.', 'Apple, Inc.', 'Meta Platforms, Inc.', 'Netflix, Inc.']  \nrating_df = sentiment_df[sentiment_df['ultimate_parent_company_name'].isin(big_boys)]      # only our tech basket\n\nrating_df = rating_df[[\"country\", \"job_category\", \"review_date_time\", \"rating_overall\"]] \n\nrating_df = rating_df[rating_df['country'] == \"United States\"]\n\nrating_df['review_date_time'] = pd.to_datetime(rating_df['review_date_time'])    #datetime conversions\nrating_df['review_date'] = rating_df['review_date_time'].dt.strftime('%Y-%m-%d')\nrating_df['review_date'] = pd.to_datetime(rating_df['review_date'])\n\nmonth_cat_ratings = rating_df.groupby(['review_date', 'job_category'])['rating_overall'].mean()  #groupby rating mean\nmonth_cat_ratings = month_cat_ratings.reset_index()\n\n#display(month_cat_ratings)\n\nstart_date = pd.to_datetime('2020-01-01') # datetime\nmonth_cat_ratings = month_cat_ratings[month_cat_ratings['review_date'] &gt;= start_date]   #weird data before 2020\nmonth_cat_ratings['days_since_2020'] = (month_cat_ratings['review_date'] - start_date).dt.days\n\n\n\n\nCode\n# Assuming 'month_cat_ratings' and 'job_category_coloring' are already defined\nfig = px.scatter(month_cat_ratings,\n                 x='days_since_2020', \n                 y='rating_overall', \n                 color='job_category',\n                 trendline=\"lowess\",\n                 color_discrete_map=job_category_coloring,\n                 title='Average Rating w.r.t. Job Category for US Employees of Big Tech Basket',\n                 labels={'rating_overall': 'Average Rating', 'days_since_2020': 'Days Since 2020'})\n\n# Convert to a graph objects figure\nfig = go.Figure(fig)\n\n# Update layout for better readability\nfig.update_layout(\n    xaxis_title='Month',\n    yaxis_title='Average Rating',\n    legend_title='Job Category',\n    xaxis_tickangle=-45,\n    width=my_width,\n    height=my_height\n)\n\nfirst_of_month = month_cat_ratings[month_cat_ratings['review_date'].dt.is_month_start]\n\nfig.update_xaxes(tickangle=45,\n                 tickmode='array',\n                 tickvals=first_of_month['days_since_2020'].tolist(),\n                 ticktext=first_of_month['review_date'].dt.strftime('%Y-%m').tolist()\n                )\n\n# Update marker size and hover info for scatter points\nfor trace in fig.data:\n    if trace.mode == 'markers':\n        trace.marker.size = 4\n        trace.hoverinfo = 'skip'\n        trace.hovertemplate = None \n\n# Update trendline attributes\nfor trace in fig.data:\n    if trace.mode == 'lines':\n        trace.line.width = 8\n        trace.showlegend = True\n\n# Create toggle buttons for trendlines\nbuttons = []\nall_lines_visible = True  # Initial state where all lines are visible\n\n# Add \"All\" button\nbuttons.append(dict(method='restyle',\n                    label='All',\n                    args=[{'visible': [True if t.mode != 'lines' or all_lines_visible else False for t in fig.data]}],\n                    args2=[{'visible': [True if t.mode != 'lines' else not all_lines_visible for t in fig.data]}]\n                    ))\n\nfor i, trace in enumerate(fig.data):\n    if trace.mode == 'lines':\n        buttons.append(dict(method='restyle',\n                            label=trace.name,\n                            args=[{'visible': [t.mode != 'lines' or t.name == trace.name for t in fig.data]}],\n                            args2=[{'visible': [t.mode != 'lines' or (t.name != trace.name and t.mode == 'lines') for t in fig.data]}]\n                            ))\n\n# Add buttons to layout\nfig.update_layout(\n    updatemenus=[\n        dict(\n            type='buttons',\n            direction='right',\n            x=0.7,\n            y=1.6,\n            showactive=True,\n            buttons=buttons\n        )\n    ]\n)\n\n# Show the figure\nfig.show()"
  },
  {
    "objectID": "posts/newsletter/Newsletter.html#should-big-tech-shareholders-care-about-these-trends",
    "href": "posts/newsletter/Newsletter.html#should-big-tech-shareholders-care-about-these-trends",
    "title": "The Tech Talent Shuffle: Redrawing Silicon Valley’s Job Map",
    "section": "4. Should Big Tech Shareholders Care about These Trends?",
    "text": "4. Should Big Tech Shareholders Care about These Trends?\nTo further explore the relevance of these job category flows to the shareholders of these companies, we investigate the (albeit in-sample for the purposes of this assignment) predictive power of flow data for each of these companies in forecasting equity returns in their respective companies with a one-month horizon (as the flow data is aggregated monthly).\nWe retained categories whose inflow and/or outflow data were significant at a 5% confidence level (net flow, lacking uniqueness beyond the inflow and outflow features, was not used). While non-rigorous, we find that: operations and marketing data offered the most consistent significance amongst our big tech basket."
  },
  {
    "objectID": "posts/newsletter/Newsletter.html#job-category-flows-and-monthly-returns",
    "href": "posts/newsletter/Newsletter.html#job-category-flows-and-monthly-returns",
    "title": "The Tech Talent Shuffle: Redrawing Silicon Valley’s Job Map",
    "section": "Job Category Flows and Monthly Returns",
    "text": "Job Category Flows and Monthly Returns\n\n\nCode\nnames = role_df.company.unique().tolist()\n#print(names)\n\n\n\n\nCode\ntickers = ['GOOGL', 'AMZN', 'AAPL', 'META', 'MSFT', 'NFLX']  # tickers for our basket\nticker_map = {'Alphabet, Inc.':'GOOGL', 'Amazon.com, Inc.':'AMZN', 'Apple, Inc.':'AAPL', 'Meta Platforms, Inc.':'META', 'Microsoft Corp.':'MSFT', 'Netflix, Inc.':'NFLX'}    # create a dictoinary to map company names to tickers\n\n\n\n\nCode\ndef get_prices(company_name):   # function to read ticker.csv from company anme\n    ticker = ticker_map[company_name]\n\n    prices = pd.read_csv(f\"{proj_directory_path}/big_tech_monthly_returns/{ticker}.csv\") # read file\n    \n    prices['Date'] = pd.to_datetime(prices['Date'], format=\"%Y-%m-%d\") # convert 'Date' to datetime\n    prices = prices[prices['Date'].dt.day == 1] #drop any rows not on first of month\n    prices = prices[prices['Date'] &gt;= \"2016-01-01\"]\n    #prices = prices.set_index('Date')\n    prices = prices[[\"Date\", 'Close']]\n    prices['Ret'] = prices['Close'].diff()/prices['Close'].shift(1)\n    prices = prices[[\"Date\", \"Ret\"]]\n    prices['Next_Ret'] = prices['Ret'].shift(-1)\n    prices = prices.dropna()\n    #display(prices)\n    \n    return prices\n\n\n\n\nCode\nnetflow_cmc = role_df[(role_df['region'] == 'Northern America') & (role_df['job_category'] != 'empty')].groupby(['month', 'company', 'job_category'])[['inflow', 'outflow', 'netflow']].sum()/100\n\npivot = netflow_cmc.pivot_table(index=['month', 'company'], columns='job_category', values=['inflow', 'outflow', 'netflow'])\n\ndef get_flows_by_category_month(company_name):    # get nicely formatted df with flow data by company\n    netflow_cmc = pivot.loc[(slice(None), company_name), :]\n    netflow_cmc = netflow_cmc.reset_index()\n    netflow_cmc = netflow_cmc.set_index('month')\n\n    # Flatten the multi-level column structure in 'meta_cats'\n    netflow_cmc.columns = ['_'.join(col).strip() for col in netflow_cmc.columns.values]\n\n    # Now join 'meta_cats' with 'meta_monthly_prices'\n    # Assuming 'Date' is a column in 'meta_monthly_prices' and you want to join on 'month' in 'meta_cats'\n    netflow_cmc.reset_index(inplace=True)  # Ensure 'month' is a column for the join\n\n    inflow_columns = netflow_cmc.filter(like='inflow_')\n\n    inflow_mean = inflow_columns.mean(axis=1)\n\n    netflow_cmc['inflow_Mean'] = inflow_mean      # add a mean column to make it \"harder\" for certain category flows to be significant\n    \n    outflow_columns = netflow_cmc.filter(like='outflow_')\n\n    outflow_mean = outflow_columns.mean(axis=1)\n\n    netflow_cmc['outflow_Mean'] = outflow_mean      # add a mean column to make it \"harder\" for certain category flows to be significant\n    \n    netflow_columns = netflow_cmc.filter(like='netflow_')\n\n    netflow_mean = netflow_columns.mean(axis=1)\n\n    netflow_cmc['netflow_Mean'] = netflow_mean      # add a mean column to make it \"harder\" for certain category flows to be significant\n    \n    # Assuming 'meta_cats' is your DataFrame\n\n#     # Apply log transformation and shift by one period\n#     X_shifted = netflow_cmc[netflow_cmc.columns[2:23]].shift(-1)\n\n#     # Rename the shifted columns to indicate they are lagged\n#     X_shifted = X_shifted.add_suffix('_lag')\n\n#     # Concatenate the lagged columns to the original DataFrame\n#     netflow_cmc = pd.concat([netflow_cmc, X_shifted], axis=1)\n\n    # Optional: Drop rows with NaN values if necessary\n    netflow_cmc.dropna(inplace=True)\n\n    prices = get_prices(company_name)\n    \n    netflow_cmc = netflow_cmc.merge(prices, left_on='month', right_on='Date', how='inner')\n    \n    return netflow_cmc\n\n\n\n\nCode\ndef run_regression(flow_return_data, flow_types):   # run regression as a function of the flow types we want, in our case \"inflow\" and \"outflow\"\n    regex_pattern = '|'.join(flow_types)\n    selected_columns = flow_return_data.filter(regex=regex_pattern).columns\n    \n    X = flow_return_data[selected_columns] # Independent variables\n    y = flow_return_data['Next_Ret']           # Dependent variable\n\n    # Create a model and fit it\n    model = sm.OLS(y, X).fit()\n\n    # View the regression results\n    res = model.summary()\n\n    res_html = res.tables[1].as_html()\n    df = pd.read_html(res_html, header=0, index_col=0)[0]\n    \n    return df\n\n\n\n\nCode\ndef get_significant_predictors(company_name, flow_types, p_value_max): # filter the regression output to return just what we want\n    \n    prices = get_prices(company_name)\n    \n    netflow_cmc = get_flows_by_category_month(company_name)\n    \n    df = run_regression(netflow_cmc, flow_types)\n    \n    df = df[df['P&gt;|t|'] &lt; p_value_max]\n    df['company'] = company_name\n    df = df[['company', 'coef', 'P&gt;|t|']]\n\n    return df\n\n\n\n\nCode\ndf = pd.DataFrame()\nfor name in names: # loop over companies and concatenate significant predictor outputs\n    output = get_significant_predictors(name, ['inflow', 'outflow'], 0.1)\n    df = pd.concat([df,output])\n    #display(output)\n    \ndf = df.reset_index()\ndf = df.rename(columns={'index':'flowtype_category'})\ndf['category'] = df['flowtype_category'].apply(lambda x: x.split('_')[1])\n\n\n\n\nCode\ncategory_order = [\"Scientist\", \"Engineer\", \"Finance\", \"Admin\", \"Sales\", \"Marketing\", \"Operations\"] # consistent category order for both tables\n\ninflow_df = df[df['flowtype_category'].str.contains('inflow')] # filter by inflow\n\n\noutflow_df = df[df['flowtype_category'].str.contains('outflow')] # filter by outflow\n\n\n\ninflow_pivot = inflow_df.pivot(index='category', columns='company', values='coef')     # create pivot tables for barcharts\ninflow_pivot = inflow_pivot.reindex(category_order)\n\noutflow_pivot = outflow_df.pivot(index='category', columns='company', values='coef')     # create pivot tables for barcharts\noutflow_pivot = outflow_pivot.reindex(category_order)\n\n\n\n\nCode\ndef plot_pivot_bar_chart(pivot_input, title):   # function to create barcharts from pivot tables\n    # Number of categories and companies\n    num_categories = len(pivot_input)\n    num_companies = len(pivot_input.columns)\n    bar_width = 0.15  # Width of the bars\n\n    # Create a figure\n    fig = go.Figure()\n\n    # Add a bar for each company\n    for i, company in enumerate(pivot_input.columns):\n        fig.add_trace(go.Bar(\n            x=pivot_input.index,\n            y=pivot_input[company],\n            name=company,\n            offsetgroup=i,\n            width=bar_width\n        ))\n\n    # Update the layout\n    fig.update_layout(\n        barmode='group',\n        xaxis=dict(title='Category', tickangle=-45),\n        yaxis=dict(title='Fcast Return', tickformat='.2%'),\n        title=title,\n        legend_title='Company',\n        showlegend=True,\n        width=1000,\n        height=600,\n    )\n\n    # Show the plot\n    #fig.show(renderer=\"notebook\")\n    fig.show()\n\nplot_pivot_bar_chart(inflow_pivot, \"Forecasted Monthly Return Changes for every 100 people hired\")\nplot_pivot_bar_chart(outflow_pivot, \"Forecasted Monthly Return Changes for every 100 people lost\")\n\n\n                                                \n\n\n                                                \n\n\nThis raises the ‘chicken or the egg’ question for our big tech companies as they navigate changes in human capital. It seems that Operations and Marketing employees play key roles in these companies. Have big tech companies been actively offloading these employees as they determined they’re not so useful? Or have they not been useful because of their experiences? (Operations employees have the lowest ratings after all).\nNonetheless, Engineers, Financiers, and Administrators seem to be “under the radar” - for now.\nIndeed, Scientist inflows seem to predict Alphabet returns - this could be evidence supporting the hiring of these professionals, as observed in Figure 2."
  },
  {
    "objectID": "posts/newsletter/Newsletter.html#so-what",
    "href": "posts/newsletter/Newsletter.html#so-what",
    "title": "The Tech Talent Shuffle: Redrawing Silicon Valley’s Job Map",
    "section": "So what?",
    "text": "So what?\nIn conclusion, we find a stark prioritization of Engineers and Scientists in big tech hiring, signaling a potential undervaluation or misutilization of Operations and Marketing employees. This shifting focus of the tech industry’s human capital acquisitions may hint at emerging career opportunities, challenges, or increased budget constraints.\n\n\nCode\n# correlation coefficient of sal vs us rank wr.t. job category grouped by year relatively stationary\n# difference in salary w.r.t. rank between big_boys and other companies very close to 0 everywhere, indicating systematic discrimination\n\n# fractionally integrated flow data likely best predictor, such that memory of the integrated series corresponds to the \"effective window\" after which flow changes have impacts on company performance;\n# as count data is non-stationary and is perpetually affected my monthly flows, and on the other hand, monthly flows are \"too recent\" to so quickly affect company performance\n# weighting flows by seniority will likely yield finer, more significant predictors\n\n#oddly, inflow and outflow correlations are not strongly inversely related, indicating that mechanisms by which certain job category workers influence company performance lie beyond that of a naive \"net human capital gain\" in that category\n\n# filling times low for all categories and salary quartiles\n\n# salaries rising for finance and engineer roles\n\n# idea to use average rating per month w.r.t rating question, and infer any relevance of certain subratings on next month flows, to infer things like people leaving due to poor management_ratings, etc.."
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "posts/new_blog_post/post.html",
    "href": "posts/new_blog_post/post.html",
    "title": "This is a dummy blog posts",
    "section": "",
    "text": "Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nam suscipit est nec dui eleifend, at dictum elit ullamcorper. Aliquam feugiat dictum bibendum. Praesent fermentum laoreet quam, cursus volutpat odio dapibus in. Fusce luctus porttitor vehicula. Donec ac tortor nisi. Donec at lectus tortor. Morbi tempor, nibh non euismod viverra, metus arcu aliquet elit, sed fringilla urna leo vel purus.\n\n\nLorem ipsum dolor sit amet, consectetur adipiscing elit. Nam suscipit est nec dui eleifend, at dictum elit ullamcorper. Aliquam feugiat dictum bibendum. Praesent fermentum laoreet quam, cursus volutpat odio dapibus in. Fusce luctus porttitor vehicula. Donec ac tortor nisi. Donec at lectus tortor. Morbi tempor, nibh non euismod viverra, metus arcu aliquet elit, sed fringilla urna leo vel purus.\n\n\nThis is inline code plus a small code chunk.\n\nlibrary(tidyverse)\n\nggplot(mpg) +\n  geom_jitter(aes(cty, hwy), size = 4, alpha = 0.5) \n\n\n\n\n\n\n\n\n\n\n\n\nTransforming OLS estimatesMaximizing likelihood\n\n\n\n\nCode\npreds_lm %&gt;% \n  ggplot(aes(body_mass_g, bill_length_mm, col = correct)) +\n  geom_jitter(size = 4, alpha = 0.6) +\n  facet_wrap(vars(species)) +\n  scale_color_manual(values = c('grey60', thematic::okabe_ito(3)[3])) +\n  scale_x_continuous(breaks = seq(3000, 6000, 1000)) +\n  theme_minimal(base_size = 12) +\n  theme(\n    legend.position = 'top', \n    panel.background = element_rect(color = 'black'),\n    panel.grid.minor = element_blank()\n  ) +\n  labs(\n    x = 'Body mass (in g)',\n    y = 'Bill length (in mm)'\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nglm.mod &lt;- glm(sex ~ body_mass_g + bill_length_mm + species, family = binomial, data = dat)\n\npreds &lt;- dat %&gt;% \n  mutate(\n    prob.fit = glm.mod$fitted.values,\n    prediction = if_else(prob.fit &gt; 0.5, 'male', 'female'),\n    correct = if_else(sex == prediction, 'correct', 'incorrect')\n  )\n\n\npreds %&gt;% \n  ggplot(aes(body_mass_g, bill_length_mm, col = correct)) +\n  geom_jitter(size = 4, alpha = 0.6) +\n  facet_wrap(vars(species)) +\n  scale_x_continuous(breaks = seq(3000, 6000, 1000)) +\n  scale_color_manual(values = c('grey60', thematic::okabe_ito(3)[3])) +\n  theme_minimal(base_size = 10) +\n  theme(\n    legend.position = 'top', \n    panel.background = element_rect(color = 'black'),\n    panel.grid.minor = element_blank()\n  ) +\n  labs(\n    x = 'Body mass (in g)',\n    y = 'Bill length (in mm)'\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\[\n\\int_0^1 f(x) \\ dx\n\\]\n\n\n\n\n\ngeom_density(\n  mapping = NULL,\n  data = NULL,\n  stat = \"density\",\n  position = \"identity\",\n  ...,\n  na.rm = FALSE,\n  orientation = NA,\n  show.legend = NA,\n  inherit.aes = TRUE,\n  outline.type = \"upper\"\n)\nstat_density(\n  mapping = NULL,\n  data = NULL,\n  geom = \"area\",\n  position = \"stack\",\n  ...,\n  bw = \"nrd0\",\n  adjust = 1,\n  kernel = \"gaussian\",\n  n = 512,\n  trim = FALSE,\n  na.rm = FALSE,\n  orientation = NA,\n  show.legend = NA,\n  inherit.aes = TRUE\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nggplot(data = gapminder::gapminder, mapping = aes(x = lifeExp, fill = continent)) +\n  stat_density(position = \"identity\", alpha = 0.5)\n\n\n\n\nBla bla bla. This is a caption in the margin. Super cool isn’t it?"
  },
  {
    "objectID": "posts/new_blog_post/post.html#merriweather",
    "href": "posts/new_blog_post/post.html#merriweather",
    "title": "This is a dummy blog posts",
    "section": "",
    "text": "Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nam suscipit est nec dui eleifend, at dictum elit ullamcorper. Aliquam feugiat dictum bibendum. Praesent fermentum laoreet quam, cursus volutpat odio dapibus in. Fusce luctus porttitor vehicula. Donec ac tortor nisi. Donec at lectus tortor. Morbi tempor, nibh non euismod viverra, metus arcu aliquet elit, sed fringilla urna leo vel purus.\n\n\nThis is inline code plus a small code chunk.\n\nlibrary(tidyverse)\n\nggplot(mpg) +\n  geom_jitter(aes(cty, hwy), size = 4, alpha = 0.5) \n\n\n\n\n\n\n\n\n\n\n\n\nTransforming OLS estimatesMaximizing likelihood\n\n\n\n\nCode\npreds_lm %&gt;% \n  ggplot(aes(body_mass_g, bill_length_mm, col = correct)) +\n  geom_jitter(size = 4, alpha = 0.6) +\n  facet_wrap(vars(species)) +\n  scale_color_manual(values = c('grey60', thematic::okabe_ito(3)[3])) +\n  scale_x_continuous(breaks = seq(3000, 6000, 1000)) +\n  theme_minimal(base_size = 12) +\n  theme(\n    legend.position = 'top', \n    panel.background = element_rect(color = 'black'),\n    panel.grid.minor = element_blank()\n  ) +\n  labs(\n    x = 'Body mass (in g)',\n    y = 'Bill length (in mm)'\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nglm.mod &lt;- glm(sex ~ body_mass_g + bill_length_mm + species, family = binomial, data = dat)\n\npreds &lt;- dat %&gt;% \n  mutate(\n    prob.fit = glm.mod$fitted.values,\n    prediction = if_else(prob.fit &gt; 0.5, 'male', 'female'),\n    correct = if_else(sex == prediction, 'correct', 'incorrect')\n  )\n\n\npreds %&gt;% \n  ggplot(aes(body_mass_g, bill_length_mm, col = correct)) +\n  geom_jitter(size = 4, alpha = 0.6) +\n  facet_wrap(vars(species)) +\n  scale_x_continuous(breaks = seq(3000, 6000, 1000)) +\n  scale_color_manual(values = c('grey60', thematic::okabe_ito(3)[3])) +\n  theme_minimal(base_size = 10) +\n  theme(\n    legend.position = 'top', \n    panel.background = element_rect(color = 'black'),\n    panel.grid.minor = element_blank()\n  ) +\n  labs(\n    x = 'Body mass (in g)',\n    y = 'Bill length (in mm)'\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\[\n\\int_0^1 f(x) \\ dx\n\\]"
  },
  {
    "objectID": "posts/new_blog_post/post.html#columns",
    "href": "posts/new_blog_post/post.html#columns",
    "title": "This is a dummy blog posts",
    "section": "",
    "text": "geom_density(\n  mapping = NULL,\n  data = NULL,\n  stat = \"density\",\n  position = \"identity\",\n  ...,\n  na.rm = FALSE,\n  orientation = NA,\n  show.legend = NA,\n  inherit.aes = TRUE,\n  outline.type = \"upper\"\n)\nstat_density(\n  mapping = NULL,\n  data = NULL,\n  geom = \"area\",\n  position = \"stack\",\n  ...,\n  bw = \"nrd0\",\n  adjust = 1,\n  kernel = \"gaussian\",\n  n = 512,\n  trim = FALSE,\n  na.rm = FALSE,\n  orientation = NA,\n  show.legend = NA,\n  inherit.aes = TRUE\n)"
  },
  {
    "objectID": "posts/new_blog_post/post.html#margin-captions",
    "href": "posts/new_blog_post/post.html#margin-captions",
    "title": "This is a dummy blog posts",
    "section": "",
    "text": "ggplot(data = gapminder::gapminder, mapping = aes(x = lifeExp, fill = continent)) +\n  stat_density(position = \"identity\", alpha = 0.5)\n\n\n\n\nBla bla bla. This is a caption in the margin. Super cool isn’t it?"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  }
]